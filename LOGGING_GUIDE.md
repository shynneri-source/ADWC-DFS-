# üìù ADWC-DFS Logging Guide

## Overview

T·∫•t c·∫£ c√°c script training (`train.py`, `evaluate.py`, `demo.py`) ƒë·ªÅu t·ª± ƒë·ªông l∆∞u log v√†o th∆∞ m·ª•c `logs/` v·ªõi t√™n file c√≥ timestamp ƒë·ªÉ d·ªÖ qu·∫£n l√Ω.

## T√≠nh NƒÉng Logging

### 1. T·ª± ƒê·ªông L∆∞u Log

M·ªói l·∫ßn ch·∫°y training/evaluation, to√†n b·ªô output t·ª´ terminal s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o file log:

```bash
# Ch·∫°y training - log t·ª± ƒë·ªông ƒë∆∞·ª£c t·∫°o
uv run train.py --sample_frac 0.1

# Output:
# üìù Log file: logs/training_20241005_143022.log
```

### 2. Format T√™n File Log

T√™n file log c√≥ format: `{prefix}_{timestamp}.log`

- **prefix:** 
  - `training` - Training script
  - `evaluation` - Evaluation script  
  - `demo` - Demo script
  
- **timestamp:** `YYYYMMDD_HHMMSS`

**V√≠ d·ª•:**
- `training_20241005_143022.log` - Training l√∫c 14:30:22 ng√†y 05/10/2024
- `evaluation_20241005_150130.log` - Evaluation l√∫c 15:01:30
- `demo_20241005_152045.log` - Demo l√∫c 15:20:45

### 3. N·ªôi Dung Log

Log file ch·ª©a:
- Timestamp b·∫Øt ƒë·∫ßu training
- Configuration parameters
- Progress c·ªßa t·ª´ng stage
- Metrics v√† results
- Error messages (n·∫øu c√≥)
- Th·ªùi gian ho√†n th√†nh

## C√°ch S·ª≠ D·ª•ng

### Ch·∫°y V·ªõi Logging (M·∫∑c ƒê·ªãnh)

```bash
# Training v·ªõi log
uv run train.py --train_path data/train.csv --test_path data/test.csv

# Evaluation v·ªõi log
uv run evaluate.py --sample_frac 0.1

# Demo v·ªõi log
uv run demo.py
```

### T·∫Øt Logging

N·∫øu kh√¥ng mu·ªën l∆∞u log (ch·∫°y nhanh h∆°n m·ªôt ch√∫t):

```bash
# Training kh√¥ng l∆∞u log
uv run train.py --no_log

# Evaluation kh√¥ng l∆∞u log
uv run evaluate.py --no_log
```

### Ch·ªâ ƒê·ªãnh Th∆∞ M·ª•c Log

```bash
# L∆∞u log v√†o th∆∞ m·ª•c kh√°c
uv run train.py --log_dir my_logs
```

## Xem Log Files

### 1. S·ª≠ D·ª•ng Script view_logs.py

#### Li·ªát K√™ T·∫•t C·∫£ Log Files

```bash
uv run view_logs.py --list

# Output:
################################################################################
                                  Log Files
################################################################################

#    Filename                                 Size         Modified
--------------------------------------------------------------------------------
1    training_20241005_143022.log            15.2 KB      2024-10-05 14:35:12
2    evaluation_20241005_150130.log          8.5 KB       2024-10-05 15:03:45
3    demo_20241005_152045.log                3.2 KB       2024-10-05 15:21:30
```

#### Xem Log M·ªõi Nh·∫•t

```bash
uv run view_logs.py --latest
```

#### Xem Log C·ª• Th·ªÉ

```bash
# Xem theo s·ªë th·ª© t·ª±
uv run view_logs.py --view 1

# Xem theo t√™n file
uv run view_logs.py --file logs/training_20241005_143022.log
```

#### Xem N D√≤ng ƒê·∫ßu/Cu·ªëi

```bash
# Xem 50 d√≤ng ƒë·∫ßu
uv run view_logs.py --latest --lines 50

# Xem 100 d√≤ng cu·ªëi
uv run view_logs.py --latest --lines 100 --tail
```

#### Filter Theo Prefix

```bash
# Ch·ªâ xem training logs
uv run view_logs.py --list --prefix training

# Xem evaluation log m·ªõi nh·∫•t
uv run view_logs.py --latest --prefix evaluation
```

#### T√¨m Ki·∫øm Trong Logs

```bash
# T√¨m keyword "error"
uv run view_logs.py --search error

# T√¨m trong training logs only
uv run view_logs.py --search "F1 Score" --prefix training

# T√¨m metrics
uv run view_logs.py --search "precision"
```

### 2. S·ª≠ D·ª•ng L·ªánh Linux

```bash
# Li·ªát k√™ logs
ls -lht logs/

# Xem log m·ªõi nh·∫•t
tail -f logs/training_*.log | tail -1

# Xem 100 d√≤ng cu·ªëi
tail -100 logs/training_20241005_143022.log

# T√¨m ki·∫øm
grep "F1 Score" logs/training_*.log

# Xem real-time (trong khi training)
tail -f logs/training_*.log
```

## V√≠ D·ª• Th·ª±c T·∫ø

### V√≠ D·ª• 1: Training v√† Xem Log

```bash
# B∆∞·ªõc 1: Ch·∫°y training
uv run train.py --sample_frac 0.1

# Output s·∫Ω hi·ªÉn th·ªã:
# üìù Log file: logs/training_20241005_143022.log

# B∆∞·ªõc 2: Sau khi training xong, xem log
uv run view_logs.py --latest
```

### V√≠ D·ª• 2: So S√°nh Nhi·ªÅu L·∫ßn Training

```bash
# Training l·∫ßn 1
uv run train.py --sample_frac 0.1 --k_neighbors 20

# Training l·∫ßn 2
uv run train.py --sample_frac 0.1 --k_neighbors 30

# Training l·∫ßn 3
uv run train.py --sample_frac 0.1 --k_neighbors 50

# Xem t·∫•t c·∫£ training logs
uv run view_logs.py --list --prefix training

# So s√°nh results
uv run view_logs.py --search "F1 Score" --prefix training
```

### V√≠ D·ª• 3: Debug L·ªói

```bash
# N·∫øu training b·ªã l·ªói
uv run train.py --sample_frac 0.5

# Xem log ƒë·ªÉ t√¨m l·ªói
uv run view_logs.py --latest --search error

# Ho·∫∑c xem to√†n b·ªô log
uv run view_logs.py --latest
```

### V√≠ D·ª• 4: Monitor Training Real-time

```bash
# Terminal 1: Ch·∫°y training
uv run train.py --train_path data/train.csv

# Terminal 2: Theo d√µi log real-time
tail -f logs/training_*.log
```

## Qu·∫£n L√Ω Log Files

### D·ªçn D·∫πp Logs C≈©

```bash
# X√≥a logs c≈© h∆°n 7 ng√†y
find logs/ -name "*.log" -mtime +7 -delete

# X√≥a logs c≈© h∆°n 30 ng√†y
find logs/ -name "*.log" -mtime +30 -delete

# Gi·ªØ l·∫°i 10 logs m·ªõi nh·∫•t, x√≥a ph·∫ßn c√≤n l·∫°i
ls -t logs/*.log | tail -n +11 | xargs rm -f
```

### Backup Logs

```bash
# Backup to√†n b·ªô logs
tar -czf logs_backup_$(date +%Y%m%d).tar.gz logs/

# Backup ch·ªâ training logs
tar -czf training_logs_$(date +%Y%m%d).tar.gz logs/training_*.log
```

### T·ªï Ch·ª©c Logs

```bash
# T·∫°o th∆∞ m·ª•c theo th√°ng
mkdir -p logs/archive/2024_10
mv logs/training_202410*.log logs/archive/2024_10/

# T·∫°o th∆∞ m·ª•c theo experiment
mkdir -p logs/experiment_1
mv logs/training_20241005_14*.log logs/experiment_1/
```

## Log Content Structure

M·ªôt log file ƒëi·ªÉn h√¨nh c√≥ c·∫•u tr√∫c:

```
================================================================================
ADWC-DFS Training Log
Started: 2024-10-05 14:30:22
================================================================================

############################################################
                 ADWC-DFS Training Pipeline                 
############################################################

Timestamp: 2024-10-05 14:30:22
Configuration:
  Train path: data/train.csv
  Test path: data/test.csv
  ...

Loading data...
[Training progress...]

============================================================
              Stage 1: Local Density Profiling              
============================================================
[Stage 1 details...]

============================================================
                 Stage 2: Cascade Training                  
============================================================
[Stage 2 details...]

============================================================
             Stage 3: Dynamic Feature Synthesis             
============================================================
[Stage 3 details...]

============================================================
             Stage 4: Meta-Classifier Training              
============================================================
[Stage 4 details...]

[Results and metrics...]

############################################################
Training completed in X.XX seconds
############################################################
```

## Tips & Best Practices

### 1. ƒê·∫∑t T√™n Experiment

Th√™m prefix m√¥ t·∫£ v√†o t√™n log:

```bash
# Kh√¥ng t·ªët
uv run train.py --sample_frac 0.1

# T·ªët h∆°n - rename log sau
mv logs/training_20241005_143022.log logs/exp1_k20_training_20241005_143022.log
```

### 2. Ghi Ch√∫ Trong Log

Th√™m comments v√†o ƒë·∫ßu training:

```bash
# T·∫°o note file
echo "Experiment: Testing k_neighbors=50 with full data" > logs/exp_notes.txt
uv run train.py --k_neighbors 50
```

### 3. Compare Multiple Runs

```bash
# Extract metrics t·ª´ logs
grep "F1 Score:" logs/training_*.log > results_comparison.txt

# Ho·∫∑c d√πng search
uv run view_logs.py --search "Key Test Metrics" --prefix training
```

### 4. Archive Important Runs

```bash
# L∆∞u l·∫°i runs quan tr·ªçng
mkdir -p logs/important_runs
cp logs/training_20241005_143022.log logs/important_runs/best_model_v1.log
```

## Troubleshooting

### Log File Kh√¥ng ƒê∆∞·ª£c T·∫°o

```bash
# Check quy·ªÅn write
ls -ld logs/

# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
mkdir -p logs
```

### Log File Qu√° L·ªõn

```bash
# Check size
du -h logs/*.log

# Compress logs c≈©
gzip logs/training_202409*.log
```

### Kh√¥ng Th·∫•y Output Trong Terminal

N·∫øu d√πng `--no_log`, output ch·ªâ ra terminal.
N·∫øu kh√¥ng d√πng `--no_log`, output v√†o c·∫£ terminal v√† file.

## Advanced Usage

### Custom Log Directory Per Experiment

```bash
# Experiment 1
uv run train.py --log_dir logs/exp1 --k_neighbors 20

# Experiment 2
uv run train.py --log_dir logs/exp2 --k_neighbors 30

# Compare
uv run view_logs.py --log_dir logs/exp1 --latest
uv run view_logs.py --log_dir logs/exp2 --latest
```

### Parse Logs Programmatically

```python
import re

def extract_metrics(log_file):
    metrics = {}
    with open(log_file) as f:
        for line in f:
            if 'F1 Score:' in line:
                metrics['f1'] = float(re.search(r'(\d+\.\d+)', line).group(1))
            if 'PR AUC:' in line:
                metrics['pr_auc'] = float(re.search(r'(\d+\.\d+)', line).group(1))
    return metrics

# Extract t·ª´ t·∫•t c·∫£ logs
import glob
for log_file in glob.glob('logs/training_*.log'):
    metrics = extract_metrics(log_file)
    print(f"{log_file}: {metrics}")
```

## Summary

**Logging t·ª± ƒë·ªông:**
- ‚úÖ M·ªçi output ƒë∆∞·ª£c l∆∞u v√†o file
- ‚úÖ Timestamp trong t√™n file
- ‚úÖ D·ªÖ d√†ng xem l·∫°i
- ‚úÖ So s√°nh multiple runs
- ‚úÖ Debug errors

**Commands:**
```bash
# Training v·ªõi log (default)
uv run train.py

# Xem logs
uv run view_logs.py --list
uv run view_logs.py --latest
uv run view_logs.py --search "keyword"

# Manage logs
ls -lht logs/
tail -f logs/training_*.log
```

**Best practice:**
1. Lu√¥n ƒë·ªÉ logging b·∫≠t (default)
2. Xem log sau m·ªói run ƒë·ªÉ verify
3. Backup logs quan tr·ªçng
4. D·ªçn d·∫πp logs c≈© ƒë·ªãnh k·ª≥

---

üìù **M·ªçi training run ƒë·ªÅu ƒë∆∞·ª£c log t·ª± ƒë·ªông - kh√¥ng lo m·∫•t th√¥ng tin!**
